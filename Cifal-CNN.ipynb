{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a68299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b045126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "import py7zr\n",
    "from io import BytesIO ## this too\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d0f6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device (GPU/CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fab30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    # Conv2d( in_channels, out_channels, kernel_size) B,3,32,32\n",
    "    nn.Conv2d(3, 64, 3, stride= 1, padding=1),  nn.BatchNorm2d(64), nn.LeakyReLU(0.1) , nn.MaxPool2d(2, 2), # B,32,16,16\n",
    "    nn.Conv2d(64, 128, 3, stride= 1, padding=1), nn.BatchNorm2d(128), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2), # B,32,8,8\n",
    "    nn.Conv2d(128, 256, 3, stride= 1, padding=1), nn.BatchNorm2d(256), nn.LeakyReLU(0.1), nn.MaxPool2d(2, 2), # B,32,4,4\n",
    "    nn.Conv2d(256, 512, 3, stride= 1, padding=1 ), nn.BatchNorm2d(512), nn.LeakyReLU(0.1),\n",
    "    nn.AdaptiveAvgPool2d(1), ##like in ResNET\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512, 512), nn.BatchNorm1d(512), nn.LeakyReLU(0.1),\n",
    "    nn.Linear(512, 10)\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=0.0005)\n",
    "scheduler = StepLR(optim, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe0cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    transforms.RandomErasing(p=0.5,scale=(0.02, 0.1),value=1.0, inplace=False),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3163831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## Train with the good old torchvision set\n",
    "\n",
    "cifar10 = torchvision.datasets.CIFAR10(root='./data', download=True, train=True, transform=transform_train)\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root='./data', download=True, train=False, transform=transform_test)\n",
    "\n",
    "batch_size = 64 ## increase\n",
    "n = batch_size\n",
    "dataloader = DataLoader(cifar10, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "dataloader_test = DataLoader(cifar10_test, batch_size=batch_size, shuffle=False, num_workers=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e95984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): LeakyReLU(negative_slope=0.1)\n",
       "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): LeakyReLU(negative_slope=0.1)\n",
       "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): LeakyReLU(negative_slope=0.1)\n",
       "  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): LeakyReLU(negative_slope=0.1)\n",
       "  (15): AdaptiveAvgPool2d(output_size=1)\n",
       "  (16): Flatten(start_dim=1, end_dim=-1)\n",
       "  (17): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (18): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): LeakyReLU(negative_slope=0.1)\n",
       "  (20): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## xavier init\n",
    "def init_weights_xavier(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)  # Apply Xavier normal initialization\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model.apply(init_weights_xavier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a83401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "losses = []\n",
    "steps = []\n",
    "losses_t = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f20fd13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8442, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8090, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4569, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4258, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4261, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(32):\n",
    "    for batch in dataloader:\n",
    "        i +=1\n",
    "        x = batch[0].to('cuda')\n",
    "        y = batch[1].to('cuda')\n",
    "        model = model.to('cuda')\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits,y)\n",
    "        optim.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        losses_t.append(loss.detach())\n",
    "        steps.append(i)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(loss)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25d32c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(steps, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "762eb1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.87712\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Loop over training dataset\n",
    "for x_batch, y_batch in dataloader:\n",
    "    logits = model(x_batch.to(\"cuda\")) # Forward pass on the mini-batch\n",
    "    loss = F.cross_entropy(logits.cpu(), y_batch) # Compute loss\n",
    "\n",
    "    # Calculate predictions for the batch\n",
    "    pred_labels = torch.max(logits, dim=1).indices\n",
    "\n",
    "    # Update total correct predictions and total predictions\n",
    "    total_correct += (y_batch == pred_labels.cpu()).sum().item()\n",
    "    total_predictions += y_batch.size(0)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = total_correct / total_predictions\n",
    "print(f\"Training Accuracy: {overall_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6c4419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.8536\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Loop over testing dataset\n",
    "for x_batch, y_batch in dataloader_test:\n",
    "    logits = model(x_batch.to(\"cuda\")) # Forward pass on the mini-batch\n",
    "    loss = F.cross_entropy(logits.cpu(), y_batch) # Compute loss\n",
    "\n",
    "    # Calculate predictions for the batch\n",
    "    pred_labels = torch.max(logits, dim=1).indices\n",
    "\n",
    "    # Update total correct predictions and total predictions\n",
    "    total_correct += (y_batch == pred_labels.cpu()).sum().item()\n",
    "    total_predictions += y_batch.size(0)\n",
    "\n",
    "# Calculate overall accuracy and print it out.\n",
    "overall_accuracy = total_correct / total_predictions\n",
    "#DON'T FORGET TO PRINT OUT YOUR TESTING ACCURACY\n",
    "print(f\"Testing Accuracy: {overall_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17e7d282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "工作目录已设置为： C:\\Users\\Frank\\Desktop\\Deep learning fundational\\Assignment 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 获取当前 notebook 文件所在的路径\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# 设置工作目录为当前文件夹\n",
    "os.chdir(current_dir)\n",
    "print(\"工作目录已设置为：\", current_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e2ead4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4688/4688 [00:08<00:00, 537.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create submission file with predictions for test images from .7z archive\n",
    "test_filenames = []\n",
    "test_images = []\n",
    "\n",
    "with py7zr.SevenZipFile('cifar-10/test.7z', mode='r') as z:\n",
    "    for name, file in z.readall().items():\n",
    "        if name.endswith('.png'):\n",
    "            img = Image.open(BytesIO(file.read()))\n",
    "            test_images.append(transform_test(img))\n",
    "            test_filenames.append(name)\n",
    "\n",
    "test_images = torch.stack(test_images)\n",
    "test_loader = DataLoader(test_images, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Prediction and CSV creation\n",
    "classes = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "result = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for inputs in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        result.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': [os.path.basename(f).replace('.png', '') for f in test_filenames],  # Remove .png from filenames\n",
    "    'label': [classes[label] for label in result]\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf65aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4175164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd34cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586c6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9bbc27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9784541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
